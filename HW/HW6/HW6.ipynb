{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW6.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN9qvmMe3bZQ8IINldNvjmr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yeb2Binfang/ECE-GY9123_DL/blob/main/HW/HW6/HW6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxK7qvDM6s2z"
      },
      "source": [
        "## A simple GAN\n",
        "In this problem, the goal is to train and visualize the outputs of a simple Deep\n",
        "Convolutional GAN (DCGAN) to generate realistic-looking (but fake) images of clothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbTWxpoC6-mj"
      },
      "source": [
        "* a. Use the FashionMNIST training dataset to train the DCGAN. APIs for downloading it\n",
        "are available in both PyTorch and TensorFlow. Images are grayscale and size 28 × 28.\n",
        "* b. Use the following discriminator architecture (kernel size = 5 × 5 with stride = 2 in both\n",
        "directions):\n",
        "** 2D convolutions (1 × 28 × 28 → 64 × 14 × 14 → 128 × 7 × 7)\n",
        "** each convolutional layer is equipped with a Leaky ReLU with slope 0.3, followed\n",
        "by Dropout with parameter 0.3.\n",
        "** a dense layer that takes the flattened output of the last convolution and maps it to a\n",
        "scalar.\n",
        "\n",
        "Here is a [link](https://discuss.pytorch.org/t/how-to-keep-the-shape-of-input-and-output-same-when-dilation-conv/14338/3) that discusses how to appropriately choose padding and stride values in order to\n",
        "desired sizes.\n",
        "\n",
        "* c. Use the following generator architecture (which is essentially the reverse of a standard\n",
        "discriminative architecture). You can use the same kernel size. Construct:\n",
        "** a dense layer that takes a unit Gaussian noise vector of length 100 and maps it to a\n",
        "vector of size 7 ∗ 7 ∗ 256. No bias terms.\n",
        "** several transpose 2D convolutions (256 × 7 × 7 → 128 × 7 × 7 → 64 × 14 × 14 →\n",
        "1 × 28 × 28). No bias terms.\n",
        "** each convolutional layer (except the last one) is equipped with Batch Normalization\n",
        "(batch norm), followed by Leaky ReLU with slope 0.3. The last (output) layer is\n",
        "equipped with tanh activation (no batch norm).\n",
        "* d. Use the cross-entropy loss for training both the generator and the discriminator. Use the\n",
        "Adam optimizer with learning rate $10^{-4}$. \n",
        "* e. Train it for 50 epochs. You can use minibatch sizes of 16, 32, or 64. Training may take\n",
        "several minutes (or even up to an hour), so be patient! Display intermediate images\n",
        "generated after T = 10, T = 30, and T = 50 epochs. If the random seeds are fixed\n",
        "throughout then you should get results of the following quality.\n",
        "![WeChat Screenshot_20210501224441](https://user-images.githubusercontent.com/68700549/116800418-dcd71480-aace-11eb-9f80-41f4ede3ec0c.png)\n",
        "* f. Report loss curves for both the discriminator and the generator loss over all epochs, and\n",
        "qualitatively comment on their behavior.\n",
        "\n",
        "https://debuggercafe.com/implementing-deep-convolutional-gan-with-pytorch/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TF6BljYF-D0w"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pirS2cNH6nM0"
      },
      "source": [
        "# import standard PyTorch modules\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter # TensorBoard support\n",
        "\n",
        "# import torchvision module to handle image manipulation\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsNditvB_LKw"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkSDp23P-5eV"
      },
      "source": [
        "## Creates a series of transformation to prepare the dataset. We convert images to tensor\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,),)])\n",
        "\n",
        "## Load the dataset\n",
        "train_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data', download=True, train=True, transform=transform)\n",
        "test_set = datasets.FashionMNIST('~/.pytorch/F_MNIST_data', download=True, train=False, transform=transform)\n",
        "\n",
        "## Data Loader\n",
        "train_data_loader = torch.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3DfMKbxET14"
      },
      "source": [
        "### Explore dataset\n",
        "We have 60,000 data  for training and 10,000 data for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ5GxXp4DcF_",
        "outputId": "3fa1f526-32ee-415f-d2f5-5bc45217f401"
      },
      "source": [
        "print(train_set)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 60000\n",
            "    Root location: /root/.pytorch/F_MNIST_data\n",
            "    Split: Train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.5,), std=(0.5,))\n",
            "           )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0w0ZuMYzEZVy",
        "outputId": "d7e0a839-d072-4da0-d0b1-15e03e90f16d"
      },
      "source": [
        "print(test_set)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset FashionMNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: /root/.pytorch/F_MNIST_data\n",
            "    Split: Test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               ToTensor()\n",
            "               Normalize(mean=(0.5,), std=(0.5,))\n",
            "           )\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfk0KgcYFfbm"
      },
      "source": [
        "### Visualize the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbHwoyepEMXA",
        "outputId": "d10771e6-3b3a-4e88-dc55-5cd8083df47a"
      },
      "source": [
        "dataiter = iter(train_data_loader)\n",
        "images, labels = dataiter.next()\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'torch.Tensor'>\n",
            "torch.Size([64, 1, 28, 28])\n",
            "torch.Size([64])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "z02X5HFbFg9e",
        "outputId": "41b65ee8-5c75-4440-a091-82c1e3cdf9c5"
      },
      "source": [
        "rand_index = np.random.randint(0,64)\n",
        "fig = plt.figure(figsize=(15,5))\n",
        "plt.imshow(images[rand_index].reshape((28,28)),cmap='gray')\n",
        "print(\"the label is: \"+ str(int(labels[rand_index])))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the label is: 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoYIOMWxEqQ1"
      },
      "source": [
        "## Build discriminator architecture\n",
        "kernel size = 5 × 5 with stride = 2 in both\n",
        "directions:\n",
        "\n",
        "* 2D convolutions (1 × 28 × 28 → 64 × 14 × 14 → 128 × 7 × 7)\n",
        "* each convolutional layer is equipped with a Leaky ReLU with slope 0.3, followed\n",
        "by Dropout with parameter 0.3.\n",
        "* a dense layer that takes the flattened output of the last convolution and maps it to a\n",
        "scalar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRwOBJUWEp3U"
      },
      "source": [
        "def discriminator_model():\n",
        "  model = nn.Sequential(\n",
        "      nn.Conv2d(1,64,5,2,padding=2),\n",
        "      nn.LeakyReLU(0.3),\n",
        "      nn.Dropout(0.3),\n",
        "\n",
        "      nn.Conv2d(64,128,5,2,padding=2),\n",
        "      nn.LeakyReLU(0.3),\n",
        "      nn.Dropout(0.3),\n",
        "\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(6272,1)\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESwFrHAqL5Mn",
        "outputId": "c726e737-4c8f-4fcd-af0e-301bf0b6f99a"
      },
      "source": [
        "from torchsummary import summary\n",
        "discriminator = discriminator_model()\n",
        "summary(discriminator,(1,28,28))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 14, 14]           1,664\n",
            "         LeakyReLU-2           [-1, 64, 14, 14]               0\n",
            "           Dropout-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4            [-1, 128, 7, 7]         204,928\n",
            "         LeakyReLU-5            [-1, 128, 7, 7]               0\n",
            "           Dropout-6            [-1, 128, 7, 7]               0\n",
            "           Flatten-7                 [-1, 6272]               0\n",
            "            Linear-8                    [-1, 1]           6,273\n",
            "================================================================\n",
            "Total params: 212,865\n",
            "Trainable params: 212,865\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.48\n",
            "Params size (MB): 0.81\n",
            "Estimated Total Size (MB): 1.29\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WT7OY8Wynx4_"
      },
      "source": [
        "class discriminator_model(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(discriminator_model, self).__init__()\n",
        "    self.main = nn.Sequential(\n",
        "      nn.Conv2d(1,64,5,2,padding=2),\n",
        "      nn.LeakyReLU(0.3),\n",
        "      nn.Dropout(0.3),\n",
        "\n",
        "      nn.Conv2d(64,128,5,2,padding=2),\n",
        "      nn.LeakyReLU(0.3),\n",
        "      nn.Dropout(0.3),\n",
        "\n",
        "      nn.Flatten(),\n",
        "      nn.Linear(6272,1)\n",
        "    )\n",
        "\n",
        "    \n",
        "  def weight_init(self):\n",
        "    for m in self._modules:\n",
        "      normal_init(self._modules[m])\n",
        "      \n",
        "  def forward(self, input):\n",
        "    return self.main(input)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRcuOQOHpUal"
      },
      "source": [
        "from torchsummary import summary\n",
        "discriminator = discriminator_model()\n",
        "#summary(discriminator,(1,28,28))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZmkBp1RksZb"
      },
      "source": [
        "## Build generator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN4EW1-2P0CG"
      },
      "source": [
        "* c. Use the following generator architecture (which is essentially the reverse of a standard\n",
        "discriminative architecture). You can use the same kernel size. Construct:\n",
        "** a dense layer that takes a unit Gaussian noise vector of length 100 (This means initial input.) and maps it to a\n",
        "vector of size 7 ∗ 7 ∗ 256.  No bias terms.\n",
        "** several transpose 2D convolutions (256 × 7 × 7 → 128 × 7 × 7 → 64 × 14 × 14 →\n",
        "1 × 28 × 28). No bias terms.\n",
        "** each convolutional layer (except the last one) is equipped with Batch Normalization\n",
        "(batch norm), followed by Leaky ReLU with slope 0.3. The last (output) layer is\n",
        "equipped with tanh activation (no batch norm)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPN9k1GPQZeD"
      },
      "source": [
        "def generator_model():\n",
        "  model = nn.Sequential(\n",
        "      nn.ConvTranspose2d(100,256,7,1,0,bias=False),\n",
        "      nn.BatchNorm2d(256),\n",
        "      nn.LeakyReLU(0.3),\n",
        "\n",
        "      nn.ConvTranspose2d(256,128,5,1,2,bias=False),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.LeakyReLU(0.3),\n",
        "\n",
        "      nn.ConvTranspose2d(128,64,6,2,2,bias=False),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.LeakyReLU(0.3),\n",
        "\n",
        "      nn.ConvTranspose2d(64,1,6,2,2,bias=False),\n",
        "      nn.Tanh()\n",
        "  )\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTjEKYdDiHEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0689ce1-a555-472f-b041-d493a591f6ec"
      },
      "source": [
        "generator = generator_model()\n",
        "summary(generator,(100,1,1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "   ConvTranspose2d-1            [-1, 256, 7, 7]       1,254,400\n",
            "       BatchNorm2d-2            [-1, 256, 7, 7]             512\n",
            "         LeakyReLU-3            [-1, 256, 7, 7]               0\n",
            "   ConvTranspose2d-4            [-1, 128, 7, 7]         819,200\n",
            "       BatchNorm2d-5            [-1, 128, 7, 7]             256\n",
            "         LeakyReLU-6            [-1, 128, 7, 7]               0\n",
            "   ConvTranspose2d-7           [-1, 64, 14, 14]         294,912\n",
            "       BatchNorm2d-8           [-1, 64, 14, 14]             128\n",
            "         LeakyReLU-9           [-1, 64, 14, 14]               0\n",
            "  ConvTranspose2d-10            [-1, 1, 28, 28]           2,304\n",
            "             Tanh-11            [-1, 1, 28, 28]               0\n",
            "================================================================\n",
            "Total params: 2,371,712\n",
            "Trainable params: 2,371,712\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 0.73\n",
            "Params size (MB): 9.05\n",
            "Estimated Total Size (MB): 9.78\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AkuJAAynr2Ie"
      },
      "source": [
        "def normal_init(m):\n",
        "  if isinstance(m, nn.ConvTranspose2d) or isinstance(m, nn.Conv2d):\n",
        "    m.weight.data.normal_(0.0, 0.02)\n",
        "    m.bias.data.zero_()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjFhZWCzpg8Z"
      },
      "source": [
        "class generator_model(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(generator_model, self).__init__()\n",
        "    self.main = nn.Sequential(\n",
        "      nn.ConvTranspose2d(100,256,7,1,0,bias=False),\n",
        "      nn.BatchNorm2d(256),\n",
        "      nn.LeakyReLU(0.3),\n",
        "\n",
        "      nn.ConvTranspose2d(256,128,5,1,2,bias=False),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.LeakyReLU(0.3),\n",
        "\n",
        "      nn.ConvTranspose2d(128,64,6,2,2,bias=False),\n",
        "      nn.BatchNorm2d(64),\n",
        "      nn.LeakyReLU(0.3),\n",
        "\n",
        "      nn.ConvTranspose2d(64,1,6,2,2,bias=False),\n",
        "      nn.Tanh()\n",
        "    )\n",
        "    \n",
        "  def weight_init(self):\n",
        "    for m in self._modules:\n",
        "      normal_init(self._modules[m])\n",
        "      \n",
        "  def forward(self, input):\n",
        "    return self.main(input)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sykxNYvp1Ys"
      },
      "source": [
        "generator = generator_model()\n",
        "#summary(generator,(100,1,1))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gzpIOO_dctK"
      },
      "source": [
        "latent = torch.randn(1,100, 1, 1)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNNx1LPZen46"
      },
      "source": [
        "im = generator(latent)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7bieyg1jll2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "3172cbc2-c5dd-4359-b7ac-5dd5ef93d1ab"
      },
      "source": [
        "plt.imshow(im[0, 0, :, :].detach(),cmap='gray')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe949f34590>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-CPlJ-yrdQR"
      },
      "source": [
        "generator.weight_init()\n",
        "discriminator.weight_init()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9yR0wEir8ay",
        "outputId": "2303475b-3fd1-428f-a265-4622a843e23c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "generator.cuda()\n",
        "discriminator.cuda()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "discriminator_model(\n",
              "  (main): Sequential(\n",
              "    (0): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (1): LeakyReLU(negative_slope=0.3)\n",
              "    (2): Dropout(p=0.3, inplace=False)\n",
              "    (3): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n",
              "    (4): LeakyReLU(negative_slope=0.3)\n",
              "    (5): Dropout(p=0.3, inplace=False)\n",
              "    (6): Flatten(start_dim=1, end_dim=-1)\n",
              "    (7): Linear(in_features=6272, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7ZAN7HMke5U"
      },
      "source": [
        "## Loss and optimizers\n",
        "Use the cross-entropy loss for training both the generator and the discriminator. Use the\n",
        "Adam optimizer with learning rate $10^{-4}$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZLY1byrk9Ag"
      },
      "source": [
        "loss = nn.BCELoss()\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# Setup Adam optimizers for both G and D\n",
        "optimizerD = optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
        "optimizerG = optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.999))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8L7LT6toON1"
      },
      "source": [
        "## Train\n",
        "e. Let 1 denotes real images and 0 denotes fake images. D_loss = real_loss + fake_loss. real_loss is from real images and fake_loss is from fake images that generated from genertaor. \n",
        "\n",
        "Train it for 50 epochs. You can use minibatch sizes of 16, 32, or 64. Training may take several minutes (or even up to an hour), so be patient! Display intermediate images generated after T = 10, T = 30, and T = 50 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eN6LcJNoP0t5"
      },
      "source": [
        "def plot_output():\n",
        "  z_ = torch.randn((5*5, 100)).view(-1, 100, 1, 1)\n",
        "  z_ = Variable(z_.cuda(), volatile = True)\n",
        "  \n",
        "  generator.eval()\n",
        "  test_images = generator(z_)\n",
        "  generator.train()\n",
        "  \n",
        "  grid_size = 5\n",
        "  fig, ax = plt.subplots(grid_size, grid_size, figsize = (5, 5))\n",
        "  for i, j in itertools.product(range(grid_size), range(grid_size)):\n",
        "    ax[i, j].get_xaxis().set_visible(False)\n",
        "    ax[i, j].get_yaxis().set_visible(False)\n",
        "  for k in range(grid_size * grid_size):\n",
        "    i = k // grid_size\n",
        "    j = k % grid_size\n",
        "    ax[i, j].cla()\n",
        "    ax[i, j].imshow(test_images[k, 0].cpu().data.numpy(),\n",
        "                    cmap = 'gray')\n",
        "  \n",
        "  plt.show()"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAlw8IjvQByP"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNGo0Ru1oVvT",
        "outputId": "74842cd6-232a-44dd-8d75-19f0a092768a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "n_epochs = 2\n",
        "for epoch in range(n_epochs):\n",
        "  loss_D = []\n",
        "  loss_G = []\n",
        "\n",
        "  for x, _ in train_data_loader:\n",
        "    # train the discriminator\n",
        "    discriminator.zero_grad()\n",
        "    mini_batch = x.size()[0]\n",
        "    # 1 denotes real, and 0 denotes fafke\n",
        "    label_real = torch.ones(mini_batch)\n",
        "    label_fake = torch.zeros(mini_batch)\n",
        "\n",
        "    # wrap a tensor\n",
        "    x = Variable(x).cuda()\n",
        "    label_real = Variable(label_real).cuda()\n",
        "    label_fake = Variable(label_fake).cuda()\n",
        "\n",
        "    # pass real image\n",
        "    D_real = discriminator(x).squeeze()\n",
        "    D_real_loss = loss(D_real, label_real)\n",
        "\n",
        "    # generate fake image via 100 random points using generator\n",
        "    noise = torch.randn((mini_batch,100)).view(-1,100,1,1)\n",
        "    noise = Variable(noise).cuda()\n",
        "    G_fake = generator(nosie)\n",
        "\n",
        "    D_fake = discriminator(G_fake).squeeze()\n",
        "    D_fake_loss = loss(D_fake, label_fake)\n",
        "    D_total_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "    D_total_loss.backward()\n",
        "    optimizerD.step()\n",
        "    loss_D.append(D_total_loss)\n",
        "\n",
        "    # train the generator, the generated image should be labeled as 1 (to fool discriminator)\n",
        "    generator.zero_grad()\n",
        "    G_real_loss = loss(D_fake, label_real)\n",
        "    G_real_loss.backward()\n",
        "    optimizerD.step()\n",
        "    loss_G.append(G_real_loss)\n",
        "  plot_output()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-58ec3d7d2532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# generate fake image via 100 random points using generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mG_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnosie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtjcsPKgUpXo"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUUpVlo-UADg",
        "outputId": "a156a642-353e-42a8-91b6-35f6e733f1f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "\n",
        "  D_losses = []\n",
        "  G_losses = []\n",
        "  \n",
        "  for X, _ in train_data_loader:\n",
        "    discriminator.zero_grad()\n",
        "    mini_batch = X.size()[0]\n",
        "    \n",
        "    y_real_ = torch.ones(mini_batch)\n",
        "    y_fake_ = torch.zeros(mini_batch)\n",
        "    \n",
        "    X = Variable(X.cuda())\n",
        "    y_real_ = Variable(y_real_.cuda())\n",
        "    y_fake_ = Variable(y_fake_.cuda())\n",
        "    \n",
        "    D_result = discriminator(X).squeeze()\n",
        "    D_real_loss = BCE_loss(D_result, y_real_)\n",
        "    \n",
        "    z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n",
        "    z_ = Variable(z_.cuda())\n",
        "    G_result = generator(z_)\n",
        "    \n",
        "    D_result = discriminator(G_result).squeeze()\n",
        "    D_fake_loss = BCE_loss(D_result, y_fake_)\n",
        "    D_fake_score = D_result.data.mean()\n",
        "    D_train_loss = D_real_loss + D_fake_loss\n",
        "    \n",
        "    D_train_loss.backward()\n",
        "    D_optimizer.step()\n",
        "    D_losses.append(D_train_loss)\n",
        "    \n",
        "    generator.zero_grad()\n",
        "    \n",
        "    z_ = torch.randn((mini_batch, 100)).view(-1, 100, 1, 1)\n",
        "    z_ = Variable(z_.cuda())\n",
        "    \n",
        "    G_result = generator(z_)\n",
        "    D_result = discriminator(G_result).squeeze()\n",
        "    G_train_loss = BCE_loss(D_result, y_real_)\n",
        "    G_train_loss.backward()\n",
        "    G_optimizer.step()\n",
        "    G_losses.append(G_train_loss)\n",
        "    \n",
        "  print('Epoch {} - loss_d: {:.3f}, loss_g: {:.3f}'.format((epoch + 1),\n",
        "                                                           torch.mean(torch.FloatTensor(D_losses)),\n",
        "                                                           torch.mean(torch.FloatTensor(G_losses))))\n",
        "                                                           \n",
        "  plot_output()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-31e1e2e64ac6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0my_fake_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0my_real_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_real_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0my_fake_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_fake_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tfFHpD8oWhI"
      },
      "source": [
        "## Loss\n",
        "f. Report loss curves for both the discriminator and the generator loss over all epochs, and qualitatively comment on their behavior."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xSytDIWoYdM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_vVojZ__4Pi"
      },
      "source": [
        "## Reference\n",
        "\n",
        "https://towardsdatascience.com/build-a-fashion-mnist-cnn-pytorch-style-efb297e22582"
      ]
    }
  ]
}